{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Parsa33033/ADM_HW3/blob/main/homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR-dxW-68OXP",
    "outputId": "f197a5c5-f6a8-4303-a0dd-7fd5215e53cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "rkVdjQff8ggD"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5F7tX0Mi81Ir"
   },
   "outputs": [],
   "source": [
    "num_of_pages = 384\n",
    "directory = \"\"\n",
    "links_file = \"anime.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXKNsnC682xk",
    "outputId": "a9369342-4bc3-41aa-e9b8-d40eb636b0d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 384/384 [02:53<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "anime = []\n",
    "\n",
    "                               \n",
    "if not os.path.isfile(links_file):\n",
    "    with open(directory + links_file, \"w\", encoding='utf-8', newline='') as fobj:\n",
    "        fobj.write(\"page,name,url\\n\")\n",
    "        for page in tqdm(range(0, num_of_pages)):\n",
    "            url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "            response = requests.get(url)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for tag in soup.find_all('tr'):\n",
    "                links = tag.find_all('a')\n",
    "                for link in links:        \n",
    "                    if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                        x = (page + 1, link.contents[0], link.get('href'))\n",
    "                        anime.append(x)\n",
    "                        writer = csv.writer(fobj)\n",
    "                        writer.writerow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_dir = directory + \"pages\"\n",
    "page_dir = pages_dir + \"/\" + \"page\"\n",
    "article = \"article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "ZYKNkYqf84p_",
    "outputId": "82b6bd03-2088-4b25-da8c-152586d6766b"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(pages_dir):\n",
    "    os.makedirs(pages_dir)\n",
    "    \n",
    "def urlEncodeNonAscii(b):\n",
    "    return re.sub('[\\x80-\\xFF]', lambda c: '%%%02x' % ord(c.group(0)), b)\n",
    "\n",
    "ds = pd.read_csv(directory + links_file)\n",
    "i = 1\n",
    "for p, u in tqdm(zip(ds.page, ds.url)):\n",
    "    d = page_dir + str(p)\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "    d = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "    \n",
    "    if not os.path.exists(d):\n",
    "#         request = requests.get(u, headers={'Cache-Control': 'no-cache', 'User-Agent': 'Mozilla/5.0'})\n",
    "        req = Request(u, headers={'User-Agent': 'XYZ/3.0'})\n",
    "        webpage = urlopen(req, timeout=100).read()\n",
    "        with open(d, \"w\", encoding='utf-8') as fobj:\n",
    "            fobj.write(webpage)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9WbbqyPD__n"
   },
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(parent):\n",
    "    return ''.join(parent.find_all(text=True, recursive=False)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_parser(article_path, soup):\n",
    "        anime_title = soup.title.text.strip()\n",
    "        anime_type = \"\"\n",
    "        try:\n",
    "            anime_type = getText(soup.find('span', text=\"Type:\").parent.a)\n",
    "        except:\n",
    "            pass\n",
    "        anime_num_episodes = \"\"\n",
    "        try:\n",
    "            anime_num_episodes = getText(soup.find('span', text=\"Episodes:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_aired = \"\"\n",
    "        try:\n",
    "            anime_aired = getText(soup.find('span', text=\"Aired:\").parent).split(\" to \")\n",
    "        except:\n",
    "            pass\n",
    "        anime_status = \"\"\n",
    "        try:\n",
    "            anime_status = getText(soup.find('span', text=\"Status:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_score = \"\"\n",
    "        try:\n",
    "            anime_score = getText(soup.find('span', text=\"Score:\").parent.find_all('span', {'class': 'score-label'})[0])\n",
    "        except:\n",
    "            pass\n",
    "        anime_users = \"\"\n",
    "        try:\n",
    "            anime_users = getText(soup.find('span', text=\"Members:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_rank = \"\"\n",
    "        try:\n",
    "            anime_rank = getText(soup.find('span', text=\"Ranked:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_popularity = \"\"\n",
    "        try:\n",
    "            anime_popularity = getText(soup.find('span', text=\"Popularity:\").parent)\n",
    "        except:\n",
    "            pass\n",
    "        anime_description = \"\"\n",
    "        try:\n",
    "            anime_description = getText(soup.find('h2', text=\"Synopsis\").parent.parent.p)\n",
    "        except:\n",
    "            pass\n",
    "        anime_related = []\n",
    "        try: \n",
    "            anime_related = list(set(map(getText,soup.find('h2', text=\"Related Anime\").parent.parent.tr.find_all(lambda tag:tag.name == \"a\" and tag.href != \"\")))) \n",
    "        except: \n",
    "            pass\n",
    "        anime_characters = []\n",
    "        try:\n",
    "            anime_characters = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[1].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "        except:\n",
    "            pass\n",
    "        anime_voices = []\n",
    "        try:\n",
    "            anime_voices = list(filter(None,list(map(lambda tr: re.split('\\n+', tr.find_all('td')[0].text.strip())[0], soup.find('h2', text=\"Characters & Voice Actors\").find_next('div').find_all('tr')))))\n",
    "        except:\n",
    "            pass\n",
    "        anime_staff = []\n",
    "        try:\n",
    "            anime_staff = list(map(lambda tr: re.split(\"\\n+\", tr.text.strip()), soup.find('h2', text=\"Staff\").find_next('div').find_all('tr')))\n",
    "        except:\n",
    "            pass\n",
    "        l = [anime_title,\n",
    "                anime_type,\n",
    "                anime_num_episodes,\n",
    "                anime_aired,\n",
    "                anime_status,\n",
    "                anime_score,\n",
    "                anime_users,\n",
    "                anime_rank,\n",
    "                anime_popularity,\n",
    "                anime_description,\n",
    "                anime_related,\n",
    "                anime_characters,\n",
    "                anime_voices,\n",
    "                anime_staff]\n",
    "        return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ---> 100 ---> pages/page2/article_100.html\n",
      "4 ---> 200 ---> pages/page4/article_200.html\n",
      "6 ---> 300 ---> pages/page6/article_300.html\n",
      "8 ---> 400 ---> pages/page8/article_400.html\n",
      "10 ---> 500 ---> pages/page10/article_500.html\n",
      "12 ---> 600 ---> pages/page12/article_600.html\n",
      "14 ---> 700 ---> pages/page14/article_700.html\n",
      "16 ---> 800 ---> pages/page16/article_800.html\n",
      "18 ---> 900 ---> pages/page18/article_900.html\n",
      "20 ---> 1000 ---> pages/page20/article_1000.html\n",
      "22 ---> 1100 ---> pages/page22/article_1100.html\n",
      "24 ---> 1200 ---> pages/page24/article_1200.html\n",
      "26 ---> 1300 ---> pages/page26/article_1300.html\n",
      "28 ---> 1400 ---> pages/page28/article_1400.html\n",
      "30 ---> 1500 ---> pages/page30/article_1500.html\n",
      "32 ---> 1600 ---> pages/page32/article_1600.html\n",
      "34 ---> 1700 ---> pages/page34/article_1700.html\n",
      "41 ---> 2000 ---> pages/page41/article_2000.html\n",
      "43 ---> 2100 ---> pages/page43/article_2100.html\n",
      "45 ---> 2200 ---> pages/page45/article_2200.html\n",
      "47 ---> 2300 ---> pages/page47/article_2300.html\n",
      "49 ---> 2400 ---> pages/page49/article_2400.html\n",
      "51 ---> 2500 ---> pages/page51/article_2500.html\n",
      "53 ---> 2600 ---> pages/page53/article_2600.html\n",
      "55 ---> 2700 ---> pages/page55/article_2700.html\n",
      "57 ---> 2800 ---> pages/page57/article_2800.html\n",
      "59 ---> 2900 ---> pages/page59/article_2900.html\n",
      "61 ---> 3000 ---> pages/page61/article_3000.html\n",
      "63 ---> 3100 ---> pages/page63/article_3100.html\n",
      "65 ---> 3200 ---> pages/page65/article_3200.html\n",
      "67 ---> 3300 ---> pages/page67/article_3300.html\n",
      "69 ---> 3400 ---> pages/page69/article_3400.html\n",
      "71 ---> 3500 ---> pages/page71/article_3500.html\n",
      "73 ---> 3600 ---> pages/page73/article_3600.html\n",
      "75 ---> 3700 ---> pages/page75/article_3700.html\n",
      "77 ---> 3800 ---> pages/page77/article_3800.html\n",
      "79 ---> 3900 ---> pages/page79/article_3900.html\n",
      "81 ---> 4000 ---> pages/page81/article_4000.html\n",
      "83 ---> 4100 ---> pages/page83/article_4100.html\n",
      "85 ---> 4200 ---> pages/page85/article_4200.html\n",
      "87 ---> 4300 ---> pages/page87/article_4300.html\n",
      "89 ---> 4400 ---> pages/page89/article_4400.html\n",
      "91 ---> 4500 ---> pages/page91/article_4500.html\n",
      "93 ---> 4600 ---> pages/page93/article_4600.html\n",
      "95 ---> 4700 ---> pages/page95/article_4700.html\n",
      "97 ---> 4800 ---> pages/page97/article_4800.html\n",
      "99 ---> 4900 ---> pages/page99/article_4900.html\n",
      "101 ---> 5000 ---> pages/page101/article_5000.html\n",
      "103 ---> 5100 ---> pages/page103/article_5100.html\n",
      "105 ---> 5200 ---> pages/page105/article_5200.html\n",
      "107 ---> 5300 ---> pages/page107/article_5300.html\n",
      "109 ---> 5400 ---> pages/page109/article_5400.html\n",
      "111 ---> 5500 ---> pages/page111/article_5500.html\n",
      "113 ---> 5600 ---> pages/page113/article_5600.html\n",
      "115 ---> 5700 ---> pages/page115/article_5700.html\n",
      "117 ---> 5800 ---> pages/page117/article_5800.html\n",
      "119 ---> 5900 ---> pages/page119/article_5900.html\n",
      "121 ---> 6000 ---> pages/page121/article_6000.html\n",
      "123 ---> 6100 ---> pages/page123/article_6100.html\n",
      "125 ---> 6200 ---> pages/page125/article_6200.html\n",
      "127 ---> 6300 ---> pages/page127/article_6300.html\n",
      "129 ---> 6400 ---> pages/page129/article_6400.html\n",
      "131 ---> 6500 ---> pages/page131/article_6500.html\n",
      "133 ---> 6600 ---> pages/page133/article_6600.html\n",
      "135 ---> 6700 ---> pages/page135/article_6700.html\n",
      "137 ---> 6800 ---> pages/page137/article_6800.html\n",
      "139 ---> 6900 ---> pages/page139/article_6900.html\n",
      "141 ---> 7000 ---> pages/page141/article_7000.html\n",
      "143 ---> 7100 ---> pages/page143/article_7100.html\n",
      "145 ---> 7200 ---> pages/page145/article_7200.html\n",
      "147 ---> 7300 ---> pages/page147/article_7300.html\n",
      "149 ---> 7400 ---> pages/page149/article_7400.html\n",
      "151 ---> 7500 ---> pages/page151/article_7500.html\n",
      "153 ---> 7600 ---> pages/page153/article_7600.html\n",
      "155 ---> 7700 ---> pages/page155/article_7700.html\n",
      "157 ---> 7800 ---> pages/page157/article_7800.html\n",
      "159 ---> 7900 ---> pages/page159/article_7900.html\n",
      "161 ---> 8000 ---> pages/page161/article_8000.html\n",
      "163 ---> 8100 ---> pages/page163/article_8100.html\n",
      "165 ---> 8200 ---> pages/page165/article_8200.html\n",
      "167 ---> 8300 ---> pages/page167/article_8300.html\n",
      "169 ---> 8400 ---> pages/page169/article_8400.html\n",
      "171 ---> 8500 ---> pages/page171/article_8500.html\n",
      "173 ---> 8600 ---> pages/page173/article_8600.html\n",
      "175 ---> 8700 ---> pages/page175/article_8700.html\n",
      "177 ---> 8800 ---> pages/page177/article_8800.html\n",
      "179 ---> 8900 ---> pages/page179/article_8900.html\n",
      "181 ---> 9000 ---> pages/page181/article_9000.html\n",
      "183 ---> 9100 ---> pages/page183/article_9100.html\n",
      "185 ---> 9200 ---> pages/page185/article_9200.html\n",
      "187 ---> 9300 ---> pages/page187/article_9300.html\n",
      "189 ---> 9400 ---> pages/page189/article_9400.html\n",
      "191 ---> 9500 ---> pages/page191/article_9500.html\n",
      "193 ---> 9600 ---> pages/page193/article_9600.html\n",
      "195 ---> 9700 ---> pages/page195/article_9700.html\n",
      "197 ---> 9800 ---> pages/page197/article_9800.html\n",
      "199 ---> 9900 ---> pages/page199/article_9900.html\n",
      "201 ---> 10000 ---> pages/page201/article_10000.html\n",
      "203 ---> 10100 ---> pages/page203/article_10100.html\n",
      "205 ---> 10200 ---> pages/page205/article_10200.html\n",
      "207 ---> 10300 ---> pages/page207/article_10300.html\n",
      "209 ---> 10400 ---> pages/page209/article_10400.html\n",
      "211 ---> 10500 ---> pages/page211/article_10500.html\n",
      "213 ---> 10600 ---> pages/page213/article_10600.html\n",
      "215 ---> 10700 ---> pages/page215/article_10700.html\n",
      "217 ---> 10800 ---> pages/page217/article_10800.html\n",
      "219 ---> 10900 ---> pages/page219/article_10900.html\n",
      "221 ---> 11000 ---> pages/page221/article_11000.html\n",
      "223 ---> 11100 ---> pages/page223/article_11100.html\n",
      "225 ---> 11200 ---> pages/page225/article_11200.html\n",
      "227 ---> 11300 ---> pages/page227/article_11300.html\n",
      "229 ---> 11400 ---> pages/page229/article_11400.html\n",
      "231 ---> 11500 ---> pages/page231/article_11500.html\n",
      "233 ---> 11600 ---> pages/page233/article_11600.html\n",
      "235 ---> 11700 ---> pages/page235/article_11700.html\n",
      "237 ---> 11800 ---> pages/page237/article_11800.html\n",
      "239 ---> 11900 ---> pages/page239/article_11900.html\n",
      "241 ---> 12000 ---> pages/page241/article_12000.html\n",
      "243 ---> 12100 ---> pages/page243/article_12100.html\n",
      "245 ---> 12200 ---> pages/page245/article_12200.html\n",
      "247 ---> 12300 ---> pages/page247/article_12300.html\n",
      "249 ---> 12400 ---> pages/page249/article_12400.html\n",
      "251 ---> 12500 ---> pages/page251/article_12500.html\n",
      "253 ---> 12600 ---> pages/page253/article_12600.html\n",
      "255 ---> 12700 ---> pages/page255/article_12700.html\n",
      "257 ---> 12800 ---> pages/page257/article_12800.html\n",
      "259 ---> 12900 ---> pages/page259/article_12900.html\n",
      "261 ---> 13000 ---> pages/page261/article_13000.html\n",
      "263 ---> 13100 ---> pages/page263/article_13100.html\n",
      "265 ---> 13200 ---> pages/page265/article_13200.html\n",
      "267 ---> 13300 ---> pages/page267/article_13300.html\n",
      "269 ---> 13400 ---> pages/page269/article_13400.html\n",
      "271 ---> 13500 ---> pages/page271/article_13500.html\n",
      "273 ---> 13600 ---> pages/page273/article_13600.html\n",
      "275 ---> 13700 ---> pages/page275/article_13700.html\n",
      "277 ---> 13800 ---> pages/page277/article_13800.html\n",
      "279 ---> 13900 ---> pages/page279/article_13900.html\n",
      "281 ---> 14000 ---> pages/page281/article_14000.html\n",
      "283 ---> 14100 ---> pages/page283/article_14100.html\n",
      "285 ---> 14200 ---> pages/page285/article_14200.html\n",
      "287 ---> 14300 ---> pages/page287/article_14300.html\n",
      "289 ---> 14400 ---> pages/page289/article_14400.html\n",
      "291 ---> 14500 ---> pages/page291/article_14500.html\n",
      "293 ---> 14600 ---> pages/page293/article_14600.html\n",
      "295 ---> 14700 ---> pages/page295/article_14700.html\n",
      "297 ---> 14800 ---> pages/page297/article_14800.html\n",
      "299 ---> 14900 ---> pages/page299/article_14900.html\n",
      "301 ---> 15000 ---> pages/page301/article_15000.html\n",
      "303 ---> 15100 ---> pages/page303/article_15100.html\n",
      "305 ---> 15200 ---> pages/page305/article_15200.html\n",
      "307 ---> 15300 ---> pages/page307/article_15300.html\n",
      "309 ---> 15400 ---> pages/page309/article_15400.html\n",
      "311 ---> 15500 ---> pages/page311/article_15500.html\n",
      "313 ---> 15600 ---> pages/page313/article_15600.html\n",
      "315 ---> 15700 ---> pages/page315/article_15700.html\n",
      "317 ---> 15800 ---> pages/page317/article_15800.html\n",
      "319 ---> 15900 ---> pages/page319/article_15900.html\n",
      "321 ---> 16000 ---> pages/page321/article_16000.html\n",
      "323 ---> 16100 ---> pages/page323/article_16100.html\n",
      "325 ---> 16200 ---> pages/page325/article_16200.html\n",
      "327 ---> 16300 ---> pages/page327/article_16300.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329 ---> 16400 ---> pages/page329/article_16400.html\n",
      "331 ---> 16500 ---> pages/page331/article_16500.html\n",
      "333 ---> 16600 ---> pages/page333/article_16600.html\n",
      "335 ---> 16700 ---> pages/page335/article_16700.html\n",
      "337 ---> 16800 ---> pages/page337/article_16800.html\n",
      "339 ---> 16900 ---> pages/page339/article_16900.html\n",
      "341 ---> 17000 ---> pages/page341/article_17000.html\n",
      "343 ---> 17100 ---> pages/page343/article_17100.html\n",
      "345 ---> 17200 ---> pages/page345/article_17200.html\n",
      "347 ---> 17300 ---> pages/page347/article_17300.html\n",
      "349 ---> 17400 ---> pages/page349/article_17400.html\n",
      "351 ---> 17500 ---> pages/page351/article_17500.html\n",
      "353 ---> 17600 ---> pages/page353/article_17600.html\n",
      "355 ---> 17700 ---> pages/page355/article_17700.html\n",
      "357 ---> 17800 ---> pages/page357/article_17800.html\n",
      "359 ---> 17900 ---> pages/page359/article_17900.html\n",
      "361 ---> 18000 ---> pages/page361/article_18000.html\n",
      "363 ---> 18100 ---> pages/page363/article_18100.html\n",
      "365 ---> 18200 ---> pages/page365/article_18200.html\n",
      "367 ---> 18300 ---> pages/page367/article_18300.html\n",
      "369 ---> 18400 ---> pages/page369/article_18400.html\n",
      "371 ---> 18500 ---> pages/page371/article_18500.html\n",
      "373 ---> 18600 ---> pages/page373/article_18600.html\n",
      "375 ---> 18700 ---> pages/page375/article_18700.html\n",
      "377 ---> 18800 ---> pages/page377/article_18800.html\n",
      "379 ---> 18900 ---> pages/page379/article_18900.html\n",
      "381 ---> 19000 ---> pages/page381/article_19000.html\n",
      "383 ---> 19100 ---> pages/page383/article_19100.html\n"
     ]
    }
   ],
   "source": [
    "num_of_docs = 19125\n",
    "i = 1\n",
    "with open('anime.tsv', 'w', encoding=\"utf-8\", newline='') as anime:\n",
    "    for p in range(1, num_of_pages + 1):\n",
    "        flag = True\n",
    "        if (flag): \n",
    "            article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "            while os.path.isfile(article_path):\n",
    "                article_path = page_dir + str(p) + \"/\" + article + \"_\" + str(i) + \".html\"\n",
    "                if os.path.isfile(article_path):\n",
    "                    if i % 100 == 0:\n",
    "                        print(p, \"--->\", i, \"--->\", article_path)\n",
    "                    with open(article_path, 'rb') as html:\n",
    "                        soup = BeautifulSoup(html, \"html.parser\")\n",
    "                        l = article_parser(article_path, soup)\n",
    "\n",
    "                        writer = csv.writer(anime, delimiter='\\t')\n",
    "                        writer.writerow(l)\n",
    "                i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18742"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv(\"anime.tsv\", delimiter='\\t')\n",
    "ds.columns = ['anime_title',\n",
    "                'anime_type',\n",
    "                'anime_num_episodes',\n",
    "                'anime_aired',\n",
    "                'anime_status',\n",
    "                'anime_score',\n",
    "                'anime_users',\n",
    "                'anime_rank',\n",
    "                'anime_popularity',\n",
    "                'anime_description',\n",
    "                'anime_related',\n",
    "                'anime_characters',\n",
    "                'anime_voices',\n",
    "                'anime_staff']\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seeking',\n",
       " 'restore',\n",
       " 'humanity',\n",
       " 'diminishing',\n",
       " 'hope',\n",
       " 'Survey',\n",
       " 'Corps',\n",
       " 'embark',\n",
       " 'mission',\n",
       " 'retake',\n",
       " 'Wall',\n",
       " 'Maria',\n",
       " 'battle',\n",
       " 'merciless',\n",
       " 'Titans',\n",
       " 'takes',\n",
       " 'stage',\n",
       " 'Returning',\n",
       " 'tattered',\n",
       " 'Shiganshina',\n",
       " 'District',\n",
       " 'home',\n",
       " 'Eren',\n",
       " 'Yeager',\n",
       " 'Corps',\n",
       " 'find',\n",
       " 'town',\n",
       " 'oddly',\n",
       " 'unoccupied',\n",
       " 'Titans',\n",
       " 'Even',\n",
       " 'outer',\n",
       " 'gate',\n",
       " 'plugged',\n",
       " 'strangely',\n",
       " 'encounter',\n",
       " 'opposition',\n",
       " 'The',\n",
       " 'mission',\n",
       " 'progresses',\n",
       " 'smoothly',\n",
       " 'Armin',\n",
       " 'Arlert',\n",
       " 'highly',\n",
       " 'suspicious',\n",
       " 'enemy',\n",
       " 'absence',\n",
       " 'discovers',\n",
       " 'distressing',\n",
       " 'signs',\n",
       " 'potential',\n",
       " 'scheme',\n",
       " 'follows',\n",
       " 'Eren',\n",
       " 'vows',\n",
       " 'back',\n",
       " 'everything',\n",
       " 'Alongside',\n",
       " 'Survey',\n",
       " 'Corps',\n",
       " 'countless',\n",
       " 'carve',\n",
       " 'path',\n",
       " 'towards',\n",
       " 'victory',\n",
       " 'uncover',\n",
       " 'secrets',\n",
       " 'locked',\n",
       " 'away',\n",
       " 'Yeager',\n",
       " 'family',\n",
       " 'basement',\n",
       " 'Written',\n",
       " 'MAL',\n",
       " 'Rewrite']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ds.loc[1].anime_description\n",
    "text_tokens = nltk.word_tokenize(sentence)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words() and word.isalnum()]\n",
    "tokens_without_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question\n",
    "\n",
    " In this problem, the goal is to maximize the sum of values contained in a list, where each value represents the time that the person who made the appointment wants to spend with the personal trainer, so you want to provide the personal trainer with a program that maximizes the total length of accepted appointments. \n",
    "The important condition is that the personal trainer needs a break between appointments and cannot accept two consecutive bookings. In addition, reservations are considered in the order in which they were made, so that the personal trainer cannot go back and accept past bookings, but must follow the chronological order. \n",
    "\n",
    "Therefore, considering the need for a break between one appointment and another and considering that past appointments cannot be accepted but it is necessary to keep to the chronological order, one solution could be to scroll through the list of bookings twice, that is once starting from the first reservation and then from the index 0, or even, and continuing with step 2 for all even indices and scroll the list a second time starting from the second reservation and then from the index 1, or odd, and proceeding with step 2 for all odd indices. \n",
    "Then add the values of the even and odd indices and compare the obtained values. The path that maximizes the sum, will be preferred. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appointments(appointments):\n",
    "    l1 = []\n",
    "    res1 = 0\n",
    "    l2 = []\n",
    "    res2 = 0\n",
    "    for i in range(len(appointments)):\n",
    "        if ((i % 2) == 0):\n",
    "            res1 += appointments[i]\n",
    "            l1.append(appointments[i])\n",
    "        else:\n",
    "            res2 += appointments[i]\n",
    "            l2.append(appointments[i])\n",
    "    if (res1 > res2):\n",
    "        return l1, res1\n",
    "    else:\n",
    "        return l2, res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list is: [30, 40, 25, 50, 30, 20] \n",
      "\n",
      "The best way to accept reservations in the given order is: [40, 50, 20] with max duration of: 110\n"
     ]
    }
   ],
   "source": [
    "mylist = [30, 40, 25, 50, 30, 20] \n",
    "print('The list is:', mylist, '\\n') \n",
    " \n",
    "print('The best way to accept reservations in the given order is:', appointments(mylist)[0], 'with max duration of:', appointments(mylist)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "homework3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
